{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "models.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eliaswalyba/digital-genius/blob/master/models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0WWMHFnCJTR",
        "colab_type": "text"
      },
      "source": [
        "# Machine Learning Engineer / Conversational AI Engineer - Technical\n",
        "In this test you will build a system that helps to classify simple conversations. This test can be treated as an opportunity to show skills and knowledge, as a learning exercise and as a prompt for further interviews in your process with DigitalGenius."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBnO_vtZ7FnX",
        "colab_type": "text"
      },
      "source": [
        "## Install required librairies\n",
        "1.   Gensim for the Word2Vec and Doc2Vec models\n",
        "2.   NLTK for text manipulation\n",
        "3.   BeautifulSoup4 for markup tags pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7yVWVUAlPZF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "8a9b6f25-5cae-408e-a12d-12b020782fa4"
      },
      "source": [
        "!pip install gensim\n",
        "!pip install nltk\n",
        "!pip install beautifulsoup4"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.8.4)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.16.4)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.9.189)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2019.6.16)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.189 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.12.189)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.2.1)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.189->boto3->smart-open>=1.2.1->gensim) (0.14)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.189->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (4.6.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfYunQPF7ea8",
        "colab_type": "text"
      },
      "source": [
        "## Import all the librairies we need for this project\n",
        "\n",
        "When you excute this cell, it will prompt an input box for setting up the NLTK library. Just type ***q*** in that input field and hit ***enter***."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0XX0yPxlPYZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "20d723b1-90e7-42fc-94eb-6ed17acc9809"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import utils\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec, Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "\n",
        "import nltk\n",
        "nltk.download()\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras import utils\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from itertools import islice\n",
        "import itertools\n",
        "import os\n",
        "import re\n",
        "\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas(desc=\"progress-bar\")\n",
        "\n",
        "from warnings import simplefilter\n",
        "simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfSw_eP38j8r",
        "colab_type": "text"
      },
      "source": [
        "## Load the data set from Github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMApfGKnlPYg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/eliaswalyba/digital-genius/master/tech_test_data.csv\")\n",
        "df = pd.concat([df.message, df.case_type], axis=1)\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "x_train, x_test, y_train, y_test = tts(df.message, df.case_type)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_f8792F9BXC",
        "colab_type": "text"
      },
      "source": [
        "## Text Pre-processing\n",
        "For this particular data set, our text cleaning step includes HTML decoding, remove stop words, change text to lower case, remove punctuation, remove bad characters, and so on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcIDlJv7DixR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_plot(index):\n",
        "    example = df[df.index == index][['message', 'case_type']].values[0]\n",
        "    if len(example) > 0:\n",
        "        print(example[0])\n",
        "        print('Case Type:', example[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnIyaNtyD-5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
        "    text = text.lower()\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "    text = BAD_SYMBOLS_RE.sub('', text)\n",
        "    text = ''.join(i for i in text if ord(i) < 128)\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uk-5kYCelPYo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ba6b89aa-b563-45ec-a290-5b42115a1055"
      },
      "source": [
        "df.message = df.message.apply(clean_text)\n",
        "print_plot(10)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "course let assist please share account number order id ill check\n",
            "Case Type: order_status\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4c8aTg59Zv_",
        "colab_type": "text"
      },
      "source": [
        "## Simple Statistical Learning Models\n",
        "In this section we are going to try out 3 simple statistical learning models(Multinomial Naive Bayes, Linear Support Vector Machines and Logistic Regression) and compare their accuracies on this dataset.\n",
        "\n",
        "To avoid code duplication we will create a model builder and use it for the rest of this part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQdeccR1M9Be",
        "colab_type": "text"
      },
      "source": [
        "### Model Builder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgkoUZbeNBow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model():\n",
        "    \n",
        "    def __init__(self, \n",
        "        classifier, \n",
        "        logreg_jobs=1, \n",
        "        logreg_c=1e5, \n",
        "        lsvm_loss='hinge', \n",
        "        lsvm_penalty='l2', \n",
        "        lsvm_alpha=1e-3, \n",
        "        lsvm_random_state=42, \n",
        "        lsvm_max_iter=5, \n",
        "        lsvm_tol=None\n",
        "    ):\n",
        "        if classifier == 'naive bayes':\n",
        "            self.classifier = MultinomialNB()\n",
        "        elif classifier == 'logistic regression':\n",
        "            self.classifier = LogisticRegression(n_jobs=logreg_jobs, C=logreg_c)\n",
        "        elif classifier == 'lsvm':\n",
        "            self.classifier = SGDClassifier(\n",
        "                loss=lsvm_loss, \n",
        "                penalty=lsvm_penalty, \n",
        "                alpha=lsvm_alpha, \n",
        "                random_state=lsvm_random_state, \n",
        "                max_iter=lsvm_max_iter, \n",
        "                tol=lsvm_tol\n",
        "            )\n",
        "        else:\n",
        "            self.classifier = None\n",
        "            \n",
        "            \n",
        "    def train(self, feature, label):\n",
        "        self.model = Pipeline([\n",
        "            ('vectorizer', CountVectorizer()),\n",
        "            ('transformer', TfidfTransformer()),\n",
        "            ('classifier', self.classifier),\n",
        "        ]).fit(feature, label)\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def test(self, test):\n",
        "        return self.model.predict(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoFT8Oz3Rn0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_score_report(score, report):\n",
        "    print(f'Accuracy\\n------------------------------------------------------\\n{score}\\n')\n",
        "    print(f'Report\\n------------------------------------------------------\\n{report}\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yrH8dCrGy0Q",
        "colab_type": "text"
      },
      "source": [
        "### Multinomial Naive Bayes Classifier\n",
        "Let's train a Naive Bayes classifier, which provides a nice baseline, to try to predict the **case type** of a **message**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtNB9651lPYu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "ff697db2-bd6e-43f0-ca50-4b8f2f758f75"
      },
      "source": [
        "y_pred = Model('naive bayes').train(x_train, y_train).test(x_test)\n",
        "score  = accuracy_score(y_pred, y_test)\n",
        "report = classification_report(y_test, y_pred, target_names=df['case_type'].unique())\n",
        "print_score_report(score, report)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy\n",
            "------------------------------------------------------\n",
            "0.7272727272727273\n",
            "\n",
            "Report\n",
            "------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "order_status       0.90      0.64      0.75        14\n",
            "cancel_order       0.58      0.88      0.70         8\n",
            "\n",
            "    accuracy                           0.73        22\n",
            "   macro avg       0.74      0.76      0.73        22\n",
            "weighted avg       0.78      0.73      0.73        22\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPPA-ldlKwHA",
        "colab_type": "text"
      },
      "source": [
        "### Linear Support Vector Machine\n",
        "Linear Support Vector Machine is widely regarded as one of the best text classification algorithms. Let's build one and try it out !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HygomMYFK_BQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "c5d78343-da08-412e-fe5a-61591e8fc91b"
      },
      "source": [
        "y_pred = Model('lsvm').train(x_train, y_train).test(x_test)\n",
        "score  = accuracy_score(y_pred, y_test)\n",
        "report = classification_report(y_test, y_pred, target_names=df['case_type'].unique())\n",
        "print_score_report(score, report)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy\n",
            "------------------------------------------------------\n",
            "0.6818181818181818\n",
            "\n",
            "Report\n",
            "------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "order_status       0.82      0.64      0.72        14\n",
            "cancel_order       0.55      0.75      0.63         8\n",
            "\n",
            "    accuracy                           0.68        22\n",
            "   macro avg       0.68      0.70      0.68        22\n",
            "weighted avg       0.72      0.68      0.69        22\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjllQaCHLXqh",
        "colab_type": "text"
      },
      "source": [
        "### Logistic Regression\n",
        "Logistic regression is a simple and easy to understand classification algorithm, and Logistic regression can be easily generalized to multiple classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRlSCfGHLtpi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "6c43057b-c1be-4468-971d-5d39b1e2bb87"
      },
      "source": [
        "y_pred = Model('logistic regression').train(x_train, y_train).test(x_test)\n",
        "score  = accuracy_score(y_pred, y_test)\n",
        "report = classification_report(y_test, y_pred, target_names=df['case_type'].unique())\n",
        "print_score_report(score, report)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy\n",
            "------------------------------------------------------\n",
            "0.6363636363636364\n",
            "\n",
            "Report\n",
            "------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "order_status       0.80      0.57      0.67        14\n",
            "cancel_order       0.50      0.75      0.60         8\n",
            "\n",
            "    accuracy                           0.64        22\n",
            "   macro avg       0.65      0.66      0.63        22\n",
            "weighted avg       0.69      0.64      0.64        22\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwr73X0oSsep",
        "colab_type": "text"
      },
      "source": [
        "## Word embedings and Neural Networks\n",
        "As you can see, following some very basic steps and using a simple linear model, we were able to reach as high as an 72% accuracy on this multi-class text classification data set.\n",
        "Using the same data set, we are going to try some advanced techniques such as word embedding and neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGgOaL-ZTQ9m",
        "colab_type": "text"
      },
      "source": [
        "### Word2vec and Logistic Regression\n",
        "Word2vec, like doc2vec, belongs to the text preprocessing phase. Specifically, to the part that transforms a text into a row of numbers. Word2vec is a type of mapping that allows words with similar meaning to have similar vector representation.\n",
        "The idea behind Word2vec is rather simple: we want to use the surrounding words to represent the target words with a Neural Network whose hidden layer encodes the word representation.\n",
        "First we load a word2vec model. It has been pre-trained by Google on a 100 billion word Google News corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmmptLgplPZA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "bb89759c-acea-459b-bc04-b74c73138ab5"
      },
      "source": [
        "wv = gensim.models.KeyedVectors.load_word2vec_format(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
        "wv.init_sims(replace=True)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hByZYrVHlPZH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "ddd568d2-88f8-4703-bf46-3bd9fd408bfa"
      },
      "source": [
        "list(islice(wv.vocab, 13030, 13050))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Memorial_Hospital',\n",
              " 'Seniors',\n",
              " 'memorandum',\n",
              " 'elephant',\n",
              " 'Trump',\n",
              " 'Census',\n",
              " 'pilgrims',\n",
              " 'De',\n",
              " 'Dogs',\n",
              " '###-####_ext',\n",
              " 'chaotic',\n",
              " 'forgive',\n",
              " 'scholar',\n",
              " 'Lottery',\n",
              " 'decreasing',\n",
              " 'Supervisor',\n",
              " 'fundamentally',\n",
              " 'Fitness',\n",
              " 'abundance',\n",
              " 'Hold']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAoHbys5Z8vb",
        "colab_type": "text"
      },
      "source": [
        "BOW based approaches that includes averaging, summation, weighted addition. The common way is to average the two word vectors. Therefore, we will follow the most common way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBEshZ756sUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_averaging(wv, words):\n",
        "    all_words, mean = set(), []\n",
        "    \n",
        "    for word in words:\n",
        "        if isinstance(word, np.ndarray):\n",
        "            mean.append(word)\n",
        "        elif word in wv.vocab:\n",
        "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
        "            all_words.add(wv.vocab[word].index)\n",
        "\n",
        "    if not mean:\n",
        "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
        "        return np.zeros(wv.vector_size,)\n",
        "\n",
        "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
        "    return mean\n",
        "\n",
        "def  word_averaging_list(wv, text_list):\n",
        "    return np.vstack([word_averaging(wv, post) for post in text_list ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crlbeC_caEIV",
        "colab_type": "text"
      },
      "source": [
        "We will tokenize the text and apply the tokenization to “post” column, and apply word vector averaging to tokenized text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDfk6U8w7MT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def w2v_tokenize_text(text):\n",
        "    tokens = []\n",
        "    for sent in nltk.sent_tokenize(text, language='english'):\n",
        "        for word in nltk.word_tokenize(sent, language='english'):\n",
        "            if len(word) < 2:\n",
        "                continue\n",
        "            tokens.append(word)\n",
        "    return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqNFIHgbZpSW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "93d4271e-dce4-447f-dbe4-f89177e2e4c1"
      },
      "source": [
        "train, test = tts(df, test_size=0.2, random_state = 42)\n",
        "\n",
        "test_tokenized = test.apply(lambda r: w2v_tokenize_text(r['message']), axis=1).values\n",
        "train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['message']), axis=1).values\n",
        "\n",
        "X_train_word_average = word_averaging_list(wv,train_tokenized)\n",
        "X_test_word_average = word_averaging_list(wv,test_tokenized)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.wv.vectors_norm instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqI7iAZpaPEL",
        "colab_type": "text"
      },
      "source": [
        "Its time to see how logistic regression classifiers performs on these word-averaging document features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aHKBB5v7XS_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "29b6af2c-1051-424b-c915-22b7c7f85af3"
      },
      "source": [
        "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
        "y_pred = logreg.fit(X_train_word_average, train['message']).predict(X_test_word_average)\n",
        "print('accuracy %s' % accuracy_score(y_pred, test.message))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrJoVkb1axYP",
        "colab_type": "text"
      },
      "source": [
        "It was disappointing, worst we have seen so far."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YHyLJyvazEk",
        "colab_type": "text"
      },
      "source": [
        "### Doc2vec and Logistic Regression\n",
        "The same idea of word2vec can be extended to documents where instead of learning feature representations for words, we learn it for sentences or documents.\n",
        "\n",
        "Doc2Vec extends the idea of word2vec, however words can only capture so much, there are times when we need relationships between documents and not just words.\n",
        "\n",
        "The way to train doc2vec model for our dataset is very similar with when we train with Doc2vec and Logistic Regression.\n",
        "\n",
        "First, we label the sentences. Gensim’s Doc2Vec implementation requires each document/paragraph to have a label associated with it. and we do this by using the TaggedDocument method. The format will be “TRAIN_i” or “TEST_i” where “i” is a dummy index of the post."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds8vzkh3-l5N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def label_sentences(corpus, label_type):\n",
        "    labeled = []\n",
        "    for i, v in enumerate(corpus):\n",
        "        label = label_type + '_' + str(i)\n",
        "        labeled.append(TaggedDocument(v.split(), [label]))\n",
        "    return labeled"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FKQ-HtAbWO4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_test, y_train, y_test = tts(df.message, df.case_type, random_state=0, test_size=0.3)\n",
        "x_train = label_sentences(x_train, 'Train')\n",
        "x_test = label_sentences(x_test, 'Test')\n",
        "all_data = x_train + x_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39bAye5Obt9-",
        "colab_type": "text"
      },
      "source": [
        "According to Gensim doc2vec tutorial, its doc2vec class was trained on the entire data, and we will do the same. Let’s have a look what the tagged document looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZCvipiQ_io6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "0c6e3085-ec0c-4302-9471-5063e75e1ef3"
      },
      "source": [
        "all_data[:5]"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TaggedDocument(words=['Hey,', 'do', 'you', 'know', 'where', 'my', 'order', 'is?'], tags=['Train_0']),\n",
              " TaggedDocument(words=['No', 'worries,', 'my', 'order', 'ID', 'is', 'BEDSW912,', 'let', 'me', 'check', 'the', 'account', 'number'], tags=['Train_1']),\n",
              " TaggedDocument(words=['Of', 'course!', 'Let', 'me', 'assist.', 'Please', 'share', 'your', 'account', 'number', 'and', 'order', 'ID', 'and', 'I’ll', 'see', 'what', 'the', 'options', 'are.'], tags=['Train_2']),\n",
              " TaggedDocument(words=['Of', 'course!', 'Let', 'me', 'assist.', 'Please', 'share', 'your', 'account', 'number', 'and', 'order', 'ID', 'and', 'I’ll', 'see', 'what', 'the', 'options', 'are.'], tags=['Train_3']),\n",
              " TaggedDocument(words=['account', 'number', '01928340'], tags=['Train_4'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mamkyrgb4cB",
        "colab_type": "text"
      },
      "source": [
        "Let's train a Doc2Vec model by varyin some it's parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mChJGA0rAJou",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "aa57b657-6f0f-4c25-dcf6-bfc54020fc62"
      },
      "source": [
        "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
        "model_dbow.build_vocab([x for x in tqdm(all_data)])\n",
        "\n",
        "for epoch in range(30):\n",
        "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
        "    model_dbow.alpha -= 0.002\n",
        "    model_dbow.min_alpha = model_dbow.alpha"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 87/87 [00:00<00:00, 7412.39it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 35562.27it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 167695.06it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 248572.51it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 158516.27it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 271708.45it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 26010.72it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 221557.04it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 246058.29it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 275191.89it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 370085.65it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 280695.73it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 249763.48it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 258430.91it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 261580.25it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 205579.97it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 26854.90it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 366002.46it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 350869.66it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 127144.41it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 144288.04it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 208636.05it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 350869.66it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 368218.41it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 213769.45it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 155942.07it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 54922.40it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 251311.60it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 216175.62it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 325807.54it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 131544.50it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5jZEfaEcGX1",
        "colab_type": "text"
      },
      "source": [
        "Next, we get vectors from trained doc2vec model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JntLLQIcATxQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
        "    vectors = np.zeros((corpus_size, vectors_size))\n",
        "    for i in range(0, corpus_size):\n",
        "        prefix = vectors_type + '_' + str(i)\n",
        "        vectors[i] = model.docvecs[prefix]\n",
        "    return vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSzP_o-TcKeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_vectors_dbow = get_vectors(model_dbow, len(x_train), 300, 'Train')\n",
        "test_vectors_dbow = get_vectors(model_dbow, len(x_test), 300, 'Test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM6CvPhpcSND",
        "colab_type": "text"
      },
      "source": [
        "Finally, we get a logistic regression model trained by the doc2vec features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spNta7L_AiKa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "3572f79b-2063-4c5a-fb1a-bcfe41562c55"
      },
      "source": [
        "y_pred = LogisticRegression(n_jobs=1, C=1e5).fit(train_vectors_dbow, y_train).predict(test_vectors_dbow)\n",
        "score = accuracy_score(y_pred, y_test)\n",
        "classification_report(y_test, y_pred,target_names=df['case_type'].unique())\n",
        "print_score_report(score, report)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy\n",
            "------------------------------------------------------\n",
            "0.6666666666666666\n",
            "\n",
            "Report\n",
            "------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "order_status       0.80      0.57      0.67        14\n",
            "cancel_order       0.50      0.75      0.60         8\n",
            "\n",
            "    accuracy                           0.64        22\n",
            "   macro avg       0.65      0.66      0.63        22\n",
            "weighted avg       0.69      0.64      0.64        22\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWAn4zSAdSHO",
        "colab_type": "text"
      },
      "source": [
        "We achieve an accuracy score of 80% which is 1% higher than SVM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyfXEe5bdWKJ",
        "colab_type": "text"
      },
      "source": [
        "### Neural Network Using Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGWG2-5PfhFv",
        "colab_type": "text"
      },
      "source": [
        "**Prepare the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU3TeYKBfkzu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_size = int(len(df) * .7)\n",
        "train_messages = df['message'][:train_size]\n",
        "train_case_types = df['case_type'][:train_size]\n",
        "\n",
        "test_messages = df['message'][train_size:]\n",
        "test_case_types = df['case_type'][train_size:]\n",
        "\n",
        "max_words = 1000\n",
        "tokenize = text.Tokenizer(num_words=max_words, char_level=False)\n",
        "tokenize.fit_on_texts(train_messages)\n",
        "\n",
        "x_train = tokenize.texts_to_matrix(train_messages)\n",
        "x_test = tokenize.texts_to_matrix(test_messages)\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(train_case_types)\n",
        "y_train = encoder.transform(train_case_types)\n",
        "y_test = encoder.transform(test_case_types)\n",
        "\n",
        "num_classes = np.max(y_train) + 1\n",
        "y_train = utils.to_categorical(y_train, num_classes)\n",
        "y_test = utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4pZScHqfmH7",
        "colab_type": "text"
      },
      "source": [
        "**Build the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bOQGadoAmLm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "3ed8ec85-ec9a-40f7-eecd-b18cd36cf3d8"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(max_words,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "              \n",
        "history = model.fit(\n",
        "    x_train, \n",
        "    y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    verbose=1,\n",
        "    validation_split=0.1\n",
        ")"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 54 samples, validate on 6 samples\n",
            "Epoch 1/2\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6724 - acc: 0.5926 - val_loss: 0.6309 - val_acc: 0.6667\n",
            "Epoch 2/2\n",
            "54/54 [==============================] - 0s 586us/step - loss: 0.5878 - acc: 0.8333 - val_loss: 0.5961 - val_acc: 0.6667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_5fhys9BMtH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}